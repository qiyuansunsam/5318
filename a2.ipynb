{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 17:15:12.104851: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-06 17:15:12.139237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 17:15:12.591927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18  36  18   5   0   3  21  37  37  37  37  21\n",
      "    3   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3 106 208 111  41  34  84 170 215 217 217 215 170\n",
      "   79  34   3   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   2  77 217 192 159 204 233 251 254 254 252 250 249\n",
      "  220 174  67   3   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  11 142 248 251 254 253 234 217 215 172 130 173\n",
      "  233 244 174  34   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  10 142 250 254 254 248 179 129 125  82  41  84\n",
      "  187 233 220  79   3   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  46 208 254 254 254 233  84   7   4   2   0   4\n",
      "   84 173 249 170  21   2   0   0   0   0]\n",
      " [  0   0   0   0   1  10 128 246 254 255 254 218  48   0   0   0   0   0\n",
      "   38 127 249 220  50   9   0   0   0   0]\n",
      " [  0   0   0   0  20  77 220 254 254 253 221 122  19   0   0   0   0   0\n",
      "   21  82 233 245 114  32   0   0   0   0]\n",
      " [  0   0   0   0  50 139 250 254 247 221  91  10   0   0   0   0   0   0\n",
      "    4  34 204 250 140  50   0   0   0   0]\n",
      " [  0   0   0   2  82 172 252 254 234 177  36   1   0   0   0   0   0   0\n",
      "    2  21 172 250 172  82   2   0   0   0]\n",
      " [  0   0   0   4 125 215 254 254 204 115   4   0   0   0   0   0   0   0\n",
      "    0   5 129 250 215 125   4   0   0   0]\n",
      " [  0   0   0   4 125 215 254 250 140  50   0   0   0   0   0   0   0   0\n",
      "    0   4 127 250 217 127   4   0   0   0]\n",
      " [  0   0   0   2  82 172 252 245 114  32   0   0   0   0   0   0   0   0\n",
      "    0   4 127 250 217 127   4   0   0   0]\n",
      " [  0   0   0   0  34 115 245 222  51   9   0   0   0   0   0   0   0   0\n",
      "    0   9 140 250 217 127   4   0   0   0]\n",
      " [  0   0   0   0   9  50 220 221  51   9   0   0   0   0   0   0   0   0\n",
      "    4  34 204 253 215 125   4   0   0   0]\n",
      " [  0   0   0   0   4  34 204 233  84  22   0   0   0   0   0   0   0   0\n",
      "    9  50 220 253 204 114   4   0   0   0]\n",
      " [  0   0   0   0   0   9 139 247 171  83   3   0   0   0   0   0   0   0\n",
      "   34 115 245 250 139  50   0   0   0   0]\n",
      " [  0   0   0   0   0   2  77 220 232 175  34   0   0   0   0   0   0   9\n",
      "   96 177 252 232  82  21   0   0   0   0]\n",
      " [  0   0   0   0   0   0  10 127 242 242 131  24   3   0   0   1  22  95\n",
      "  219 246 247 159  22   3   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  33 163 218 231 171  84  39  37  52 159 232\n",
      "  253 253 207  47   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   8  90 159 229 230 172 129 127 141 220 251\n",
      "  253 245 159  22   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   7  22  95 218 247 245 245 248 251 249\n",
      "  218 163  35   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   8  77 125 127 129 170 170 127\n",
      "   77  33   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   4   4   5  20  20   5\n",
      "    2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "    return dataset['data'], dataset['labels']\n",
    "\n",
    "data_path = 'EMNIST_Byclass_Small/'\n",
    "X_train, y_train = load_data(data_path+'emnist_train.pkl')\n",
    "X_test, y_test = load_data(data_path+'emnist_test.pkl')\n",
    "\n",
    "print(X_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (100000, 28, 28, 1), Training labels shape: (100000, 62)\n",
      "Test data shape: (20000,), Test labels shape: (20000, 62)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "num_classes = 62 \n",
    "train_labels = to_categorical(y_train, num_classes)\n",
    "test_labels = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {y_test.shape}, Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "loss=\"sparse_categorical_crossentropy\"\n",
    "#loss=\"categorical_crossentropy\"\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 62\n",
    "hyperparameters = {\n",
    "    'hidden_size' : 64,\n",
    "    'dropout_rate' : 0.3,\n",
    "    'learning_rate' : 1e-4,\n",
    "    'activation' : 'relu',\n",
    "    'output_activation' : 'softmax'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(inputs, filters, kernel_size=3, stride=1, dropout_rate=0.3, activation='relu'):\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel_size, strides=stride, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x) \n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if stride != 1 or inputs.shape[-1] != filters:\n",
    "        inputs = layers.Conv2D(filters, kernel_size=1, strides=stride, padding='same')(inputs)\n",
    "        inputs = layers.BatchNormalization()(inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x) \n",
    "\n",
    "    x = layers.Add()([x, inputs])\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet(input_shape, num_classes, hyperparameters):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    hidden_size = hyperparameters['hidden_size']\n",
    "    activation= hyperparameters['activation']\n",
    "    \n",
    "    # Initial convolution layer\n",
    "    x = layers.Conv2D(hidden_size, (3, 3), padding='same', strides=1)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    x = residual_block(x, hidden_size, dropout_rate=dropout_rate, activation=activation)\n",
    "    x = residual_block(x, hidden_size*2, stride=2, dropout_rate=dropout_rate, activation=activation)  # downsample\n",
    "    x = residual_block(x, hidden_size*4, stride=2, dropout_rate=dropout_rate, activation=activation)  # downsample\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(hidden_size*8, activation=activation)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation=hyperparameters['output_activation'])(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    optimizer=Adam(learning_rate=hyperparameters['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, num_classes, hyperparameters):\n",
    "    hidden_size = hyperparameters['hidden_size']\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    activation= hyperparameters['activation']\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(hidden_size, (3, 3))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(dropout_rate-0.1)(x)\n",
    "\n",
    "    x = layers.Conv2D(hidden_size*2, (3, 3))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Conv2D(hidden_size*4, (3, 3))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(dropout_rate+0.1)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = layers.Dense(hidden_size)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=hyperparameters['output_activation'])(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    optimizer=Adam(learning_rate=hyperparameters['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, num_classes, hidden_size):\n",
    "    hidden_size = hyperparameters['hidden_size']\n",
    "    dropout_rate = hyperparameters['dropout_rate']\n",
    "    activation= hyperparameters['activation']\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Flatten the input (28, 28, 1) -> (784,)\n",
    "    x = layers.Flatten(input_shape=input_shape)(inputs)\n",
    "\n",
    "    x = layers.Dense(hidden_size*8)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Dropout(dropout_rate-0.1)(x)\n",
    "\n",
    "    x = layers.Dense(hidden_size*4)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(hidden_size*2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Dropout(dropout_rate+0.1)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=hyperparameters['output_activation'])(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    optimizer=Adam(learning_rate=hyperparameters['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 17:15:13.679394: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.701946: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.701984: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.704568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.704602: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.704623: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.867394: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.867436: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.867443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-06 17:15:13.867470: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-06 17:15:13.867490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728195316.712849   84408 service.cc:145] XLA service 0x7f8fc0001480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728195316.712899   84408 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2024-10-06 17:15:16.778556: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-06 17:15:17.072662: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728195319.583950   84466 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_4086', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "I0000 00:00:1728195319.590245   84470 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_4086', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1728195319.618054   84474 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_4086', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1728195320.260089   84465 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_3966', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   5/2500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 27ms/step - accuracy: 0.0314 - loss: 4.1815 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728195325.423758   84408 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - accuracy: 0.3694 - loss: 2.5082 - val_accuracy: 0.7398 - val_loss: 0.8204\n",
      "Epoch 2/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7434 - loss: 0.8254 - val_accuracy: 0.8235 - val_loss: 0.5323\n",
      "Epoch 3/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7938 - loss: 0.6248 - val_accuracy: 0.8282 - val_loss: 0.5016\n",
      "Epoch 4/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.8125 - loss: 0.5512 - val_accuracy: 0.8403 - val_loss: 0.4695\n",
      "Epoch 5/20\n",
      "\u001b[1m 221/2500\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.8285 - loss: 0.5135"
     ]
    }
   ],
   "source": [
    "model_res = build_resnet(input_shape, num_classes, hyperparameters)\n",
    "history = model_res.fit(\n",
    "    X_train, y_train,                 \n",
    "    validation_split=0.2,   \n",
    "    epochs=epochs,                        \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epsilon/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.3289 - loss: 2.7847 - val_accuracy: 0.6598 - val_loss: 1.2142\n",
      "Epoch 2/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6417 - loss: 1.2533 - val_accuracy: 0.7253 - val_loss: 0.8955\n",
      "Epoch 3/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.6977 - loss: 0.9835 - val_accuracy: 0.7551 - val_loss: 0.7842\n",
      "Epoch 4/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7293 - loss: 0.8536 - val_accuracy: 0.7721 - val_loss: 0.6992\n",
      "Epoch 5/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7522 - loss: 0.7736 - val_accuracy: 0.7811 - val_loss: 0.6603\n",
      "Epoch 6/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7644 - loss: 0.7110 - val_accuracy: 0.7935 - val_loss: 0.6202\n",
      "Epoch 7/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7701 - loss: 0.6892 - val_accuracy: 0.8016 - val_loss: 0.5972\n",
      "Epoch 8/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7836 - loss: 0.6462 - val_accuracy: 0.8040 - val_loss: 0.5769\n",
      "Epoch 9/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7902 - loss: 0.6254 - val_accuracy: 0.8122 - val_loss: 0.5580\n",
      "Epoch 10/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8010 - loss: 0.5947 - val_accuracy: 0.8132 - val_loss: 0.5481\n",
      "Epoch 11/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8026 - loss: 0.5829 - val_accuracy: 0.8176 - val_loss: 0.5314\n",
      "Epoch 12/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8086 - loss: 0.5604 - val_accuracy: 0.8201 - val_loss: 0.5279\n",
      "Epoch 13/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8097 - loss: 0.5509 - val_accuracy: 0.8222 - val_loss: 0.5154\n",
      "Epoch 14/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8124 - loss: 0.5362 - val_accuracy: 0.8233 - val_loss: 0.5119\n",
      "Epoch 15/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8141 - loss: 0.5338 - val_accuracy: 0.8253 - val_loss: 0.5056\n",
      "Epoch 16/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.5134 - val_accuracy: 0.8274 - val_loss: 0.5012\n",
      "Epoch 17/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8219 - loss: 0.5077 - val_accuracy: 0.8290 - val_loss: 0.4963\n",
      "Epoch 18/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8236 - loss: 0.4995 - val_accuracy: 0.8297 - val_loss: 0.4932\n",
      "Epoch 19/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8256 - loss: 0.4914 - val_accuracy: 0.8280 - val_loss: 0.4935\n",
      "Epoch 20/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.4822 - val_accuracy: 0.8306 - val_loss: 0.4865\n"
     ]
    }
   ],
   "source": [
    "model_cnn = build_cnn(input_shape, num_classes, hyperparameters)\n",
    "history = model_cnn.fit(\n",
    "    X_train, y_train,                 \n",
    "    validation_split=0.2,   \n",
    "    epochs=epochs,                        \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epsilon/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.3284 - loss: 2.8211 - val_accuracy: 0.6636 - val_loss: 1.2684\n",
      "Epoch 2/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6053 - loss: 1.4698 - val_accuracy: 0.7255 - val_loss: 0.9408\n",
      "Epoch 3/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6636 - loss: 1.1659 - val_accuracy: 0.7577 - val_loss: 0.7952\n",
      "Epoch 4/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7048 - loss: 0.9942 - val_accuracy: 0.7771 - val_loss: 0.7176\n",
      "Epoch 5/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7282 - loss: 0.9021 - val_accuracy: 0.7892 - val_loss: 0.6654\n",
      "Epoch 6/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7475 - loss: 0.8215 - val_accuracy: 0.7990 - val_loss: 0.6244\n",
      "Epoch 7/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.7676 - val_accuracy: 0.8030 - val_loss: 0.6059\n",
      "Epoch 8/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7675 - loss: 0.7283 - val_accuracy: 0.8108 - val_loss: 0.5818\n",
      "Epoch 9/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7785 - loss: 0.6928 - val_accuracy: 0.8120 - val_loss: 0.5731\n",
      "Epoch 10/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7855 - loss: 0.6635 - val_accuracy: 0.8182 - val_loss: 0.5526\n",
      "Epoch 11/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7904 - loss: 0.6386 - val_accuracy: 0.8211 - val_loss: 0.5438\n",
      "Epoch 12/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7959 - loss: 0.6144 - val_accuracy: 0.8205 - val_loss: 0.5386\n",
      "Epoch 13/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8015 - loss: 0.5985 - val_accuracy: 0.8241 - val_loss: 0.5270\n",
      "Epoch 14/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8054 - loss: 0.5810 - val_accuracy: 0.8268 - val_loss: 0.5197\n",
      "Epoch 15/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8111 - loss: 0.5658 - val_accuracy: 0.8274 - val_loss: 0.5146\n",
      "Epoch 16/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8114 - loss: 0.5545 - val_accuracy: 0.8293 - val_loss: 0.5077\n",
      "Epoch 17/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8164 - loss: 0.5401 - val_accuracy: 0.8292 - val_loss: 0.5060\n",
      "Epoch 18/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8186 - loss: 0.5291 - val_accuracy: 0.8303 - val_loss: 0.5054\n",
      "Epoch 19/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8192 - loss: 0.5215 - val_accuracy: 0.8312 - val_loss: 0.4996\n",
      "Epoch 20/20\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8258 - loss: 0.5114 - val_accuracy: 0.8317 - val_loss: 0.5030\n"
     ]
    }
   ],
   "source": [
    "model_mlp = build_mlp(input_shape, num_classes, hyperparameters)\n",
    "history = model_mlp.fit(\n",
    "    X_train, y_train,                \n",
    "    validation_split=0.2,   \n",
    "    epochs=epochs,                        \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
